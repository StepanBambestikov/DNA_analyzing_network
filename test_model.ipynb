{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5996a1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset_maker'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdataset_maker\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mds\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdata_loader\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mloaders_handler\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnetwork_model_service\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn_service\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'dataset_maker'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataset_maker as ds\n",
    "import data_loader as loaders_handler\n",
    "import network_model_service as nn_service\n",
    "import plot_manager as pl\n",
    "\n",
    "import network_classes as networks\n",
    "import parameters as constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bbbdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = networks.SimpleNetwork()\n",
    "nn_model.load_state_dict(torch.load(\"dH_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a00fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "butch_size = 64\n",
    "epoch_number = 400\n",
    "dataset = ds.get_dataset_from_excel_file(\"ML_Stepan.xlsx\", label_column_number=constant.dH_column_number,\n",
    "                                             begin_feature_column=constant.begin_feature_column,\n",
    "                                             end_feature_column=constant.end_feature_column,\n",
    "                                             dna_to_numeric_strategy=None, first_row=constant.first_row)\n",
    "train_loader, val_loader = loaders_handler.get_train_and_val_loaders(dataset, butch_size)\n",
    "butch, ground_truth_labels = next(iter(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e989920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-113.1516, -115.2000],\n",
       "        [ -50.6165,  -59.6000],\n",
       "        [ -94.5640,  -89.9000],\n",
       "        [ -57.1960,  -57.7000],\n",
       "        [ -56.9722,  -60.7000],\n",
       "        [ -61.2815,  -63.5000],\n",
       "        [ -63.3834,  -60.0000],\n",
       "        [-112.1501, -104.4000],\n",
       "        [ -50.3690,  -49.2000],\n",
       "        [ -62.4384,  -61.6000],\n",
       "        [ -64.2676,  -60.4000],\n",
       "        [ -60.9432,  -60.6000],\n",
       "        [ -55.1495,  -50.2000],\n",
       "        [ -61.9482,  -68.8000],\n",
       "        [-117.7092, -104.4000],\n",
       "        [ -57.9408,  -57.0000],\n",
       "        [ -62.4384,  -61.6000],\n",
       "        [ -61.5580,  -62.6000],\n",
       "        [ -63.9127,  -66.2000],\n",
       "        [ -60.9432,  -67.9000],\n",
       "        [ -89.1698,  -93.7000],\n",
       "        [ -64.2676,  -71.0000],\n",
       "        [ -64.1019,  -67.5000],\n",
       "        [ -66.5025,  -64.1000],\n",
       "        [ -59.0515,  -57.6000],\n",
       "        [ -47.7542,  -38.6000],\n",
       "        [ -49.1345,  -50.7000],\n",
       "        [ -52.7683,  -51.7000],\n",
       "        [ -61.9129,  -58.5000],\n",
       "        [ -71.2245,  -67.4000],\n",
       "        [ -58.5914,  -56.8000],\n",
       "        [ -46.0148,  -40.4000],\n",
       "        [ -50.8072,  -47.0000],\n",
       "        [ -62.9529,  -63.2000],\n",
       "        [ -63.5667,  -62.6000],\n",
       "        [ -61.9482,  -61.4000],\n",
       "        [ -63.1394,  -58.7000],\n",
       "        [-115.5567, -119.2000],\n",
       "        [ -71.6347,  -76.3000],\n",
       "        [ -62.1766,  -62.8000],\n",
       "        [ -60.7476,  -61.4000],\n",
       "        [ -62.4384,  -64.7000],\n",
       "        [ -61.2815,  -59.3000],\n",
       "        [ -65.4588,  -62.3000],\n",
       "        [ -59.5418,  -57.9000],\n",
       "        [ -58.4830,  -59.8000],\n",
       "        [ -45.1036,  -42.5000],\n",
       "        [ -43.6715,  -37.0000],\n",
       "        [ -58.7746,  -60.0000],\n",
       "        [ -44.6638,  -43.1000],\n",
       "        [ -95.6411, -102.8000],\n",
       "        [-110.9854, -116.1000],\n",
       "        [ -54.6157,  -54.6000],\n",
       "        [ -71.2245,  -70.1000],\n",
       "        [ -65.0675,  -60.9000],\n",
       "        [ -48.7425,  -46.3000],\n",
       "        [-116.9941, -123.4000],\n",
       "        [ -70.2760,  -74.7000],\n",
       "        [ -62.9529,  -61.4000],\n",
       "        [ -58.7785,  -58.3000]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = nn_model(butch)\n",
    "torch.mean(torch.abs(prediction - ground_truth_labels))\n",
    "torch.concatenate((prediction, ground_truth_labels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import data_loader as loader\n",
    "import network_service as nn_service\n",
    "import prediction_analyzer as pl\n",
    "\n",
    "import network_classes as networks\n",
    "import excel_parameters as constant\n",
    "import pandas as pd\n",
    "import loss\n",
    "import butch_handlers\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "# butch_size = 64\n",
    "    butch_size = 64\n",
    "    dG_coefficients_multiplier = loader.make_coefficients_multiplier(loader.dG_analytic_coefficients)\n",
    "    dataset = loader.get_dataset_from_excel_file(\"ML_Stepan.xlsx\", label_column_number=constant.dG_column_number,\n",
    "                                                 begin_feature_column=constant.begin_feature_column,\n",
    "                                                 end_feature_column=constant.end_feature_column,\n",
    "                                                 is_dna_analysis=False,\n",
    "                                                 data_handler=dG_coefficients_multiplier,\n",
    "                                                 first_row=constant.first_row)\n",
    "    #dna data\n",
    "    # dataset = loader.get_dataset_from_excel_file(\"ML_Stepan.xlsx\", label_column_number=constant.dH_column_number,\n",
    "    #                                          begin_feature_column=constant.dna_feature_column_number,\n",
    "    #                                          end_feature_column=None,\n",
    "    #                                          dna_to_numeric_strategy=loader.make_2d_data_from_text_dna,\n",
    "    #                                          first_row=constant.first_row)\n",
    "    # train service\n",
    "    train_loader, val_loader = loader.get_train_and_val_loaders(dataset, butch_size)\n",
    "    epoch_number = 5000\n",
    "    nn_model = networks.linear_classifier()\n",
    "    # loss_function = loss.complementary_normalized_l1_loss(complement_coefficient=0.1)\n",
    "    loss_function = loss.normalized_l1_loss\n",
    "    # butch_handler = butch_handlers.complementarity_butch_manager(\n",
    "    #     butch_size=butch_size, complementarity_additive_size=16,\n",
    "    #     complementary_pair_maker=butch_handlers.complementary_pair_maker_from_2d)\n",
    "    butch_handler = butch_handlers.butch_noise_adder(mean=0, sigma=0.0, noise_ground_truth=True, noise_butch=False)\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "    history_manager = nn_service.train_history_manager(epoch_number)\n",
    "    epoch_painter = nn_service.epoch_painter(drawing_function=print, epoch_number_per_drawing=10)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # reduce lr every n epochs\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')  # reduce lr on a plateu\n",
    "    is_convolution_training = False\n",
    "    train_manager = nn_service.network_train_service(nn_model, loss_function, optimizer, scheduler,\n",
    "                                                     butch_handler=butch_handler,\n",
    "                                                     is_convolution_training=is_convolution_training,\n",
    "                                                     is_relative_error=True)\n",
    "    # training\n",
    "    nn_service.train_model(train_manager, train_loader, val_loader, epoch_number, epoch_painter, history_manager)\n",
    "    # print(np.min(val_error_history[val_error_history > 0]))\n",
    "    print(history_manager.get_min_val_error())\n",
    "    nn_service.plot_train_history(*history_manager.get_train_history())\n",
    "\n",
    "    # testing\n",
    "    test_features, test_labels = dataset.tensors\n",
    "    if is_convolution_training:\n",
    "        prediction = nn_model(test_features[:, None, :])\n",
    "    else:\n",
    "        prediction = nn_model(test_features)\n",
    "    absolute_error = nn_service.get_prediction_error(prediction, test_labels, is_relative_error=False,\n",
    "                                                     is_absolute_value=False)\n",
    "    relative_error = nn_service.get_prediction_error(prediction, test_labels, is_relative_error=True,\n",
    "                                                     is_absolute_value=False)\n",
    "    pl.draw_error_histograms(\"some network\", pl.Graph(\"absolute error\", absolute_error), pl.Graph(\"relative error\", relative_error))\n",
    "    pl.draw_correlation(\"some network\", pl.CorrelationPair(prediction, test_labels, \"test\"))\n",
    "\n",
    "    # class train_features:\n",
    "    #     def __init__(self, network_name, loss_name):\n",
    "    #         self.network_name = network_name\n",
    "    #         self.loss_name = loss_name\n",
    "    #\n",
    "    #     def __hash__(self):\n",
    "    #         return hash((self.network_name, self.loss_name))\n",
    "    #\n",
    "    #     def __eq__(self, other):\n",
    "    #         return (self.network_name, self.loss_name) == (self.network_name, self.loss_name)\n",
    "    #\n",
    "    #\n",
    "    # def prediction_draw(prediction, test_labels, graph_name):\n",
    "    #     absolute_error = nn_service.get_prediction_error(prediction, test_labels, is_relative_error=False,\n",
    "    #                                                      is_absolute_value=False)\n",
    "    #     relative_error = nn_service.get_prediction_error(prediction, test_labels, is_relative_error=True,\n",
    "    #                                                      is_absolute_value=False)\n",
    "    #     pl.draw_error_histograms(graph_name, pl.Graph(\"absolute error\", absolute_error), pl.Graph(\"relative error\", relative_error))\n",
    "    #     pl.draw_correlation(graph_name, pl.CorrelationPair(prediction, test_labels, \"test\"))\n",
    "    #\n",
    "    #\n",
    "    # # nearest neighbour testing\n",
    "    # dataset = loader.get_dataset_from_excel_file(\"ML_Stepan.xlsx\", label_column_number=constant.dG_column_number,\n",
    "    #                                              begin_feature_column=constant.dna_feature_column_number,\n",
    "    #                                              end_feature_column=None,\n",
    "    #                                              is_dna_analysis=True,\n",
    "    #                                              data_handler=loader.make_2d_data_from_text_dna,\n",
    "    #                                              first_row=constant.first_row)\n",
    "    # butch_size = 64\n",
    "    # test_features, test_labels = dataset.tensors\n",
    "    # train_loader, val_loader = loader.get_train_and_val_loaders(dataset, butch_size)\n",
    "    # epoch_number = 5000\n",
    "    # networks = {\"dna2d_lc\": networks.dna_2d_linear_classifier(),\n",
    "    #             \"dna2d_two_layers\": networks.dna_2d_two_layer_network(), \"dna2d_conv net\": networks.ConvNetwork2d(), }\n",
    "    #\n",
    "    # losses = {\"normalized_l1_loss\": loss.normalized_l1_loss, \"normalized_l2_loss\": loss.normalized_l2_loss}\n",
    "    # networks_error = {}\n",
    "    # for current_network_name, current_network in networks.items():\n",
    "    #     for current_loss_name, current_loss in losses.items():\n",
    "    #         network_features = train_features(current_network_name, current_loss_name)\n",
    "    #         print(current_network_name, current_loss_name)\n",
    "    #         optimizer = optim.Adam(current_network.parameters(), lr=0.001)\n",
    "    #         history_manager = nn_service.train_history_manager(epoch_number)\n",
    "    #         scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    #         is_convolution_training = False\n",
    "    #         if current_network_name == \"dna2d_conv net\":\n",
    "    #             is_convolution_training = True\n",
    "    #         train_manager = nn_service.network_train_service(current_network, current_loss, optimizer, scheduler,\n",
    "    #                                                          butch_handler=None,\n",
    "    #                                                          is_convolution_training=is_convolution_training,\n",
    "    #                                                          is_relative_error=True)\n",
    "    #         nn_service.train_model(train_manager, train_loader, val_loader, epoch_number, epoch_painter=None,\n",
    "    #                                history_manager=history_manager)\n",
    "    #         print(\"network trained\")\n",
    "    #         train_error, val_error = history_manager.train_val_min_error()\n",
    "    #         networks_error[network_features] = train_error, val_error\n",
    "    #         if is_convolution_training:\n",
    "    #             prediction = current_network(test_features[:, None, :])\n",
    "    #         else:\n",
    "    #             prediction = current_network(test_features)\n",
    "    #         prediction_draw(prediction, test_labels, current_network_name + \" \" + current_loss_name)\n",
    "    #\n",
    "    # for network_features, error_value in networks_error.items():\n",
    "    #     print(network_features.network_name, network_features.loss_name, \" \", error_value)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
